{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control-Reacher\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will be guided through the steps to create an agent, load the train weight for the actor, and see the performance of a trained agent. This notebook only works for MacOSX, for other operation systems please change the name of envirnoment accordingly.\n",
    "\n",
    "Remember to change the kernel to 'drlnd', which can be set up following the [here](https://github.com/udacity/deep-reinforcement-learning#dependencies)\n",
    "\n",
    "## 1. Before we start\n",
    "\n",
    "Before we get started, please make sure that all necessary files are in the same folder as this notebook. Please also make sure that they are not in other sub-folders. The requirements are:\n",
    "    * actor_checkpoint.pth, which is the trained weights for actor;s network\n",
    "    * infrastructures.py\n",
    "    * agents.py\n",
    "    * the environment file, please make sure the name is 'Reacher'\n",
    "    \n",
    "\n",
    "## 2. Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# start environment, you might need to change the name\n",
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "Size of each state: 33\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('Size of each state:', state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load .py files\n",
    "\n",
    "For the following two blocks, you will have to run each of them twice. The first time will load the .py files into this notebook. The second time will actually execute the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "# %load infrastructures.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using device {}'.format(device))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        \n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        #Extract information from memory unit and return\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)    \n",
    "    \n",
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden=[400, 300]):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], action_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden[0])\n",
    "        self.bn2 = nn.BatchNorm1d(hidden[1])\n",
    "        self.initialize()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(F.relu(self.fc1(x)))\n",
    "        x = self.bn2(F.relu(self.fc2(x)))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc1.bias.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc2.bias.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        self.fc3.bias.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "class Critic1(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden=[400, 300]):\n",
    "        super(Critic1, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0]+action_size, hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], 1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden[0])\n",
    "        self.initialize()\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = torch.cat([x, action], dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc1.bias.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc2.bias.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-4, 3e-4)\n",
    "        self.fc3.bias.data.uniform_(-3e-4, 3e-4)\n",
    "        \n",
    "class Critic2(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden=[400, 300]):\n",
    "        super(Critic2, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1]+action_size, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden[0])\n",
    "        self.bn2 = nn.BatchNorm1d(hidden[1])\n",
    "        self.initialize()\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        x = self.bn1(F.relu(self.fc1(state)))\n",
    "        x = self.bn2(F.relu(self.fc2(x)))\n",
    "        x = torch.cat([x, action], dim=1)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc1.bias.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc2.bias.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-4, 3e-4)\n",
    "        self.fc3.bias.data.uniform_(-3e-4, 3e-4)\n",
    "        \n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, agent_num, action_size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones((agent_num, action_size))\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "        self.shape = (agent_num, action_size)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(self.shape[0]*self.shape[1])]).reshape(self.shape)\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "# %load agents.py\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "from infrastructures import Actor, Critic1, Critic2, ReplayBuffer, OUNoise, device\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class DDPG_Agent:\n",
    "    \n",
    "    def __init__(self, env, critic, lr1=0.0001, lr2=0.001, tau=0.001, speed1=1, speed2=1,\\\n",
    "                 step=1, learning_time=1, batch_size=64):\n",
    "        \n",
    "        #Initialize environment\n",
    "        brain_name = env.brain_names[0]\n",
    "        brain = env.brains[brain_name]\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        num_agents = len(env_info.agents)\n",
    "        action_size = brain.vector_action_space_size\n",
    "        states = env_info.vector_observations\n",
    "        state_size = states.shape[1]\n",
    "        self.env = env\n",
    "        \n",
    "        #Initialize some hyper parameters of agent\n",
    "        self.lr1 = lr1\n",
    "        self.lr2 = lr2\n",
    "        self.tau = tau\n",
    "        self.speed1 = speed1\n",
    "        self.speed2 = speed2\n",
    "        self.learning_time = learning_time\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_agents = num_agents\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = 0.99\n",
    "        self.step = step\n",
    "        \n",
    "        #Initialize agent (networks, replyabuffer and noise)\n",
    "        self.actor_local = Actor(self.state_size, self.action_size).to(device)\n",
    "        self.actor_target = Actor(self.state_size, self.action_size).to(device)\n",
    "        if critic==1:\n",
    "            self.critic_local = Critic1(self.state_size, self.action_size).to(device)\n",
    "            self.critic_target = Critic1(self.state_size, self.action_size).to(device)\n",
    "        else:\n",
    "            self.critic_local = Critic2(self.state_size, self.action_size).to(device)\n",
    "            self.critic_target = Critic2(self.state_size, self.action_size).to(device)\n",
    "        self.soft_update(self.actor_local, self.actor_target, 1)\n",
    "        self.soft_update(self.critic_local, self.critic_target, 1)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=self.lr1)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=self.lr2)\n",
    "        self.memory = ReplayBuffer(self.action_size, buffer_size=int(1e6), batch_size=self.batch_size,\\\n",
    "                                   seed=random.randint(1, self.batch_size))\n",
    "        self.noise = OUNoise(agent_num=self.num_agents, action_size=self.action_size, seed=random.randint(1, self.batch_size))\n",
    "        \n",
    "    def act(self, state, i):\n",
    "        state = torch.tensor(state, dtype=torch.float).to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).detach().cpu().numpy()\n",
    "        self.actor_local.train()\n",
    "        noise = self.noise.sample()\n",
    "        action += noise/np.sqrt(i)\n",
    "        action = np.clip(action, -1, 1)\n",
    "        return action\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "    def learn(self):\n",
    "        \n",
    "        experiences = self.memory.sample()\n",
    "        states, actions, scores, next_states, dones = experiences\n",
    "        \n",
    "        expected_rewards = scores + (1-dones)*self.gamma*self.critic_target(next_states, self.actor_target(next_states))\n",
    "        \n",
    "        for _ in range(self.speed1):\n",
    "            observed_rewards = self.critic_local(states, actions)\n",
    "            L = F.mse_loss(expected_rewards, observed_rewards)\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            L.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "            self.critic_optimizer.step()\n",
    "            del L\n",
    "        \n",
    "        for _ in range(self.speed2):\n",
    "            L = -self.critic_local(states, self.actor_local(states)).mean()\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            L.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            del L\n",
    "        \n",
    "        self.soft_update(self.actor_local, self.actor_target, self.tau)\n",
    "        self.soft_update(self.critic_local, self.critic_target, self.tau)\n",
    "        \n",
    "            \n",
    "    def train(self, n_episodes):\n",
    "        rewards = []\n",
    "        brain_name = self.env.brain_names[0]\n",
    "        score_window = deque(maxlen=100)\n",
    "        \n",
    "        for i in range(1, n_episodes+1):\n",
    "            episodic_reward = np.zeros(self.num_agents)\n",
    "            env_info = self.env.reset(train_mode=True)[brain_name]\n",
    "            states = env_info.vector_observations\n",
    "            actions = self.act(states, i)\n",
    "            t=0\n",
    "            \n",
    "            while True:\n",
    "                env_info = self.env.step(actions)[brain_name]\n",
    "                next_states = env_info.vector_observations\n",
    "                dones = env_info.local_done\n",
    "                scores = env_info.rewards\n",
    "                episodic_reward += np.array(scores)\n",
    "                for state, action, score, next_state, done in zip(states, actions, scores, next_states, dones):\n",
    "                    self.memory.add(state, action, score, next_state, done)\n",
    "                t += 1\n",
    "                \n",
    "                if len(self.memory.memory)>self.batch_size:\n",
    "                    if t % self.step == 0:\n",
    "                        for _ in range(self.learning_time):\n",
    "                            self.learn()\n",
    "                \n",
    "                if any(dones):\n",
    "                    break\n",
    "                            \n",
    "                states = next_states\n",
    "                actions = self.act(states, i)\n",
    "            score_window.append(np.mean(episodic_reward))\n",
    "            rewards.append(episodic_reward)\n",
    "            \n",
    "            print('\\rEpisode {}. Total score for this episode: {:.4f}, average score {:.4f}'.format(i, np.mean(episodic_reward),np.mean(score_window)),end='')\n",
    "            if i % 100 == 0:\n",
    "                print('')\n",
    "        \n",
    "        np.save('./offline/offline_rewards.npy',np.array(rewards))\n",
    "        self.actor_local.cpu()\n",
    "        self.critic_local.cpu()\n",
    "        torch.save(self.actor_local.state_dict(),'./offline/actor_checkpoint.pth')\n",
    "        torch.save(self.critic_local.state_dict(),'./offline/critic_checkpoint.pth')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. See the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDPG_Agent(env=env, critic=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor_local.load_state_dict(torch.load('actor_checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actor(\n",
       "  (fc1): Linear(in_features=33, out_features=400, bias=True)\n",
       "  (fc2): Linear(in_features=400, out_features=300, bias=True)\n",
       "  (fc3): Linear(in_features=300, out_features=4, bias=True)\n",
       "  (bn1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.actor_local.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 37.369499164726584\n",
      "Total number of steps: 1001\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "t = 0                                                  # count how many time steps are there\n",
    "while True:\n",
    "    t += 1\n",
    "    actions = agent.actor_local(torch.tensor(states,dtype=torch.float)).detach().cpu().numpy() # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
