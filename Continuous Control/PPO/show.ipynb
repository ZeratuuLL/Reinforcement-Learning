{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control-Reacher\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will be guided through the steps to create an agent, load the train weight for the actor, and see the performance of a trained agent. This notebook only works for MacOSX, for other operation systems please change the name of envirnoment accordingly.\n",
    "\n",
    "Remember to change the kernel to 'drlnd', which can be set up following the [here](https://github.com/udacity/deep-reinforcement-learning#dependencies)\n",
    "\n",
    "## 1. Before we start\n",
    "\n",
    "Before we get started, please make sure that all necessary files are in the same folder as this notebook. Please also make sure that they are not in other sub-folders. The requirements are:\n",
    "    * ppo_checkpoint.pth, which is the trained weights for the agent's network\n",
    "    * infrastructures.py\n",
    "    * agents.py\n",
    "    * the environment file, please make sure the name is 'Reacher'\n",
    "    \n",
    "\n",
    "## 2. Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rewards = np.load('rewards_history.npy')\n",
    "averages = [np.mean(rewards[range(0,max(1,i-100))]) for i in range(1,1501)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1276"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(averages)>=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1500-1276-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start environment, you might need to change the name\n",
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('Size of each state:', state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load .py files\n",
    "\n",
    "For the following two blocks, you will have to run each of them twice. The first time will load the .py files into this notebook. The second time will actually execute the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load infrastructures.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load agents.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. See the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'environment': {\n",
    "        'state_size':  env_info.vector_observations.shape[1],\n",
    "        'action_size': brain.vector_action_space_size,\n",
    "        'number_of_agents': len(env_info.agents)\n",
    "    },\n",
    "    'pytorch': {\n",
    "        'device': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    },\n",
    "    'hyperparameters': {\n",
    "        'discount_rate': 0.99,\n",
    "        'tau': 0.95,\n",
    "        'gradient_clip': 5,\n",
    "        'rollout_length': 2048,\n",
    "        'optimization_epochs': 10,\n",
    "        'ppo_clip': 0.2,\n",
    "        'log_interval': 2048,\n",
    "        'max_steps': 1e5,\n",
    "        'mini_batch_number': 32,\n",
    "        'entropy_coefficent': 0.01,\n",
    "        'episode_count': 250,\n",
    "        'hidden_size': 512,\n",
    "        'adam_learning_rate': 3e-4,\n",
    "        'adam_epsilon': 1e-5\n",
    "    }\n",
    "}\n",
    "    \n",
    "policy = PPOPolicyNetwork(config)\n",
    "optimizier = optim.Adam(policy.parameters(), config['hyperparameters']['adam_learning_rate'], eps=config['hyperparameters']['adam_epsilon'])\n",
    "agent = PPOAgent(env, brain_name, policy, optimizier, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.network.load_state_dict(torch.load('ppo_checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]  # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)                                               # count how many time steps are there\n",
    "while True:\n",
    "    actions, _, _, _ = agent.network(states).detach().cpu().numpy()           # select an action (for each agent)\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
