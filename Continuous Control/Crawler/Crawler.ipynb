{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "Congratulations for completing the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program!  In this notebook, you will learn how to control an agent in a more challenging environment, where the goal is to train a creature with four arms to walk forward.  **Note that this exercise is optional!**\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: CrawlerBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 129\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 20\n",
      "        Vector Action descriptions: , , , , , , , , , , , , , , , , , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "env = UnityEnvironment(file_name='Crawler.x86_64')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using device {}'.format(device))\n",
    "    \n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units, fc2_units):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1_bn = nn.BatchNorm1d(state_size)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2_bn = nn.BatchNorm1d(fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "\n",
    "        self.fc2a_bn = nn.BatchNorm1d(fc2_units)\n",
    "        self.fc2a = nn.Linear(fc2_units, fc2_units)\n",
    "\n",
    "        self.fc3_bn = nn.BatchNorm1d(fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "\n",
    "        self.fc2a.weight.data.uniform_(*hidden_init(self.fc2a))\n",
    "\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(self.fc1_bn(state)))\n",
    "        x = F.relu(self.fc2(self.fc2_bn(x)))\n",
    "\n",
    "        x = F.relu(self.fc2a(self.fc2a_bn(x)))\n",
    "\n",
    "        return F.tanh(self.fc3(self.fc3_bn(x)))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, seed, fc1_units, fc2_units):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1_bn = nn.BatchNorm1d(state_size)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2_bn = nn.BatchNorm1d(fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "\n",
    "        self.fc2a_bn = nn.BatchNorm1d(fc2_units)\n",
    "        self.fc2a = nn.Linear(fc2_units, fc2_units)\n",
    "\n",
    "        self.fc3_bn = nn.BatchNorm1d(fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "\n",
    "        self.fc2a.weight.data.uniform_(*hidden_init(self.fc2a))\n",
    "\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an critic (value) network that maps states -> value.\"\"\"\n",
    "        x = F.relu(self.fc1(self.fc1_bn(state)))\n",
    "        x = F.relu(self.fc2(self.fc2_bn(x)))\n",
    "\n",
    "        x = F.relu(self.fc2a(self.fc2a_bn(x)))\n",
    "\n",
    "        return self.fc3(self.fc3_bn(x))\n",
    "\n",
    "class PPO_Actor_Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=1024, fc2_units=1024):\n",
    "        super(PPO_Actor_Critic, self).__init__()\n",
    "        self.actor = Actor(state_size, action_size, seed, fc1_units, fc2_units)\n",
    "        self.critic = Critic(state_size, seed, fc1_units, fc2_units)  \n",
    "        self.std = nn.Parameter(torch.ones(1, action_size)*0.15)\n",
    "\n",
    "    def forward(self, state, action=None, scale=1.):\n",
    "        \"\"\"Build Policy.\n",
    "        \n",
    "        Returns\n",
    "        ======\n",
    "            action (Tensor): predicted action or inputed action\n",
    "            log_prob (Tensor): log probability of current action distribution\n",
    "            ent (Tensor): entropy of current action distribution\n",
    "            value (Tensor): estimate value function\n",
    "        \"\"\"\n",
    "        action_mean = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        \n",
    "        dist = torch.distributions.Normal(action_mean, F.hardtanh(self.std, min_val=0.06*scale, max_val=0.6*scale))\n",
    "\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        log_prob = torch.sum(log_prob, dim=1, keepdim=True)\n",
    "\n",
    "        ent = dist.entropy().mean()\n",
    "        \n",
    "        return action, log_prob, ent, value\n",
    "\n",
    "class PPO_Agent:\n",
    "    def __init__(self, env, lr=0.0001, beta=0, learning_time=5, eps=0.2, tau=0.95, batch_size=128, constraint = 1.0):\n",
    "        \n",
    "        #Initialize environment\n",
    "        brain_name = env.brain_names[0]\n",
    "        brain = env.brains[brain_name]\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        num_agents = len(env_info.agents)\n",
    "        action_size = brain.vector_action_space_size\n",
    "        states = env_info.vector_observations\n",
    "        state_size = states.shape[1]\n",
    "        self.env = env\n",
    "        random_seed = random.randint(1, 100)\n",
    "        \n",
    "        #Initialize some hyper parameters of agent\n",
    "        self.lr = lr\n",
    "        self.learning_time = learning_time\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_agents = num_agents\n",
    "        self.gamma = 0.99\n",
    "        self.batch_size = batch_size\n",
    "        self.beta = beta#parameter for entropy panelty\n",
    "        self.eps = eps#parameter for clip\n",
    "        self.tau = tau#parameter for GAE\n",
    "        \n",
    "        #Networks and optimizers\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.network = PPO_Actor_Critic(state_size, action_size, random_seed, fc1_units=1024, fc2_units=1024).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(),lr=self.lr, eps=1e-8, weight_decay=1e-4)\n",
    "        \n",
    "        self.best = -100 # This saves the best average score over 10 episodes in past agents.\n",
    "        \n",
    "        self.constraint = constraint\n",
    "        \n",
    "    def act(self, states):\n",
    "        self.network.eval()\n",
    "        with torch.no_grad():\n",
    "            states = torch.tensor(states, dtype=torch.float).to(device)\n",
    "            actions, log_probs, _, v = self.network(states)\n",
    "            actions = actions.detach().cpu().numpy()\n",
    "            log_probs = log_probs.detach().cpu().numpy()\n",
    "            v = v.detach().cpu().numpy()\n",
    "        self.network.train()\n",
    "        return actions, log_probs, v\n",
    "    \n",
    "    def check(self):\n",
    "        brain_name = self.env.brain_names[0]\n",
    "        self.network.eval()\n",
    "        total_rewards = 0\n",
    "        reward_flag = False\n",
    "        for _ in range(20):\n",
    "            scores = np.zeros(self.num_agents)\n",
    "            env_info = self.env.reset(train_mode=True)[brain_name]\n",
    "            for t in range(1500):\n",
    "                states = env_info.vector_observations\n",
    "                action, _, _ = self.act(states)\n",
    "                env_info = self.env.step(action)[brain_name]\n",
    "                rewards = np.array(env_info.rewards)\n",
    "                if any(np.isnan(rewards.reshape(-1))):\n",
    "                    rewards[np.isnan(rewards)] = -5\n",
    "                    reward_flag = True\n",
    "                scores += rewards\n",
    "            total_rewards += np.mean(scores)\n",
    "        if reward_flag:\n",
    "            print('\\n NaN in rewards during testing!')\n",
    "        self.network.train()\n",
    "        self.network.cpu()\n",
    "        if total_rewards/20 > self.best:\n",
    "            torch.save(self.network.state_dict(), 'Crawler_Checkpoint.pth')\n",
    "            self.best = total_rewards/20\n",
    "            print('  Current network average: {:.4f}. Network Updated. Current best score {:.4f}'.format(total_rewards/20, self.best))\n",
    "        else:\n",
    "            self.network.load_state_dict(torch.load('Crawler_Checkpoint.pth'))\n",
    "            print('  Current network average: {:.4f}. Network Reload. Current best score {:.4f}'.format(total_rewards/20, self.best))\n",
    "        self.network.to(device)    \n",
    "    \n",
    "    def learn(self, states, actions, log_probs, dones, Advantages, Returns):\n",
    "        '''\n",
    "        This functions calculates the clipped surrogate function and do one step update to the action network\n",
    "        And then the critic network will be updated\n",
    "        The inputs are all lists of tensors. They are already sent to device before into the list\n",
    "        '''\n",
    "        \n",
    "        #Generate dataset for getting small batches\n",
    "        mydata = TensorDataset(states, actions, log_probs, dones, Advantages, Returns)\n",
    "        Loader = DataLoader(mydata, batch_size = min(self.batch_size, len(mydata)//64), shuffle = True)\n",
    "        \n",
    "        for i in range(self.learning_time):\n",
    "            for sampled_states, sampled_actions, sampled_log_probs, sampled_dones, sampled_advantages, sampled_returns in iter(Loader):\n",
    "                _, new_log_probs, entropy, V = self.network(sampled_states, sampled_actions)\n",
    "                ratio = (new_log_probs - sampled_log_probs).exp()\n",
    "                \n",
    "                Actor_Loss = -torch.min(input=ratio*sampled_advantages, other=torch.clamp(ratio, 1-self.eps, 1+self.eps)*sampled_advantages).mean()\n",
    "                Entropy_Loss = -self.beta * entropy.mean()\n",
    "                Critic_Loss = 0.5*(V-sampled_returns).pow(2).mean()\n",
    "                Loss = Actor_Loss+Critic_Loss+Entropy_Loss\n",
    "                self.optimizer.zero_grad()\n",
    "                Loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), 1)\n",
    "                self.optimizer.step()\n",
    "            \n",
    "    def train(self, n_episode, max_t=1500, standardlize='row', method='MC', load=False):\n",
    "        '''\n",
    "        This function do the training part of the agent. The procedure is like:\n",
    "            1. initialize environment\n",
    "            2. Go through a whole episode, recode all states, actions, log_probs, rewards and dones information\n",
    "            3. call learn function to update the networks\n",
    "            4. repeat 2-3 for n_episode times.\n",
    "        '''\n",
    "        \n",
    "        if load:\n",
    "            self.network.cpu()\n",
    "            self.network.load_state_dict(torch.load('Crawler_Checkpoint.pth'))\n",
    "            self.network.to(device)\n",
    "            \n",
    "        all_rewards = []\n",
    "        total_window = deque(maxlen=100)\n",
    "        brain_name = self.env.brain_names[0]\n",
    "        score_window = deque(maxlen=100)\n",
    "        \n",
    "        method = method.upper()\n",
    "        if method not in ['MC', 'TD']:\n",
    "            print('method can be only TD or MC. Input not supported! Use TD by default')\n",
    "            method = 'TD'\n",
    "        standardlize = standardlize.lower()\n",
    "        if standardlize not in ['row', 'whole', 'none']:\n",
    "            print('Standarlization in row or as a whole or none. Input not supported. Use row instead')\n",
    "            standardlize = 'row'\n",
    "            \n",
    "        self.check()\n",
    "            \n",
    "        states_history = []\n",
    "        actions_history = []\n",
    "        rewards_history = []\n",
    "        log_probs_history = []\n",
    "        dones_history = []\n",
    "        values_history = []\n",
    "        \n",
    "        for i in range(1, n_episode+1):\n",
    "            #initialize environment\n",
    "            env_info = self.env.reset(train_mode=True)[brain_name]\n",
    "            \n",
    "            total = np.zeros(self.num_agents)#Saves every reward signal\n",
    "            scores_recorder = []#Saves the total reward whenever an agent is 'done'\n",
    "            episodic_scores = np.zeros(self.num_agents)#Saves the current cumulated reward. Reset to 0 when agent is 'done'\n",
    "            \n",
    "            states = env_info.vector_observations\n",
    "            actions, log_probs, v = self.act(states)\n",
    "            \n",
    "            reward_flag = False\n",
    "            \n",
    "            for _ in range(max_t):\n",
    "                states_history.append(torch.tensor(states, dtype=torch.float).to(device))\n",
    "                actions_history.append(torch.tensor(actions, dtype=torch.float).to(device))\n",
    "                values_history.append(torch.tensor(v, dtype=torch.float).to(device))\n",
    "                log_probs_history.append(torch.tensor(log_probs, dtype=torch.float).to(device))#Save as columns\n",
    "                if any(np.isnan(actions.reshape(-1))):\n",
    "                    print('\\nCurrent episode {}. NaN in action!'.format(i))\n",
    "                    self.network.cpu()\n",
    "                    torch.save(self.network.state_dict(), 'Crawler_Checkpoint_NaN_Action.pth')\n",
    "                    return None\n",
    "                env_info = self.env.step(actions)[brain_name]\n",
    "                next_states = env_info.vector_observations\n",
    "                dones = torch.tensor(env_info.local_done, dtype=torch.float).view(-1,1).to(device)\n",
    "                rewards = np.array(env_info.rewards)\n",
    "                if any(np.isnan(rewards.reshape(-1))):\n",
    "                    rewards[np.isnan(rewards)] = -5\n",
    "                    reward_flag = True\n",
    "                #Save reward info before turned into tensor\n",
    "                total += rewards\n",
    "                episodic_scores += rewards \n",
    "                \n",
    "                rewards = torch.tensor(rewards, dtype=torch.float).view(-1,1).to(device)\n",
    "                rewards_history.append(rewards)\n",
    "                dones_history.append(dones)\n",
    "                \n",
    "                states = next_states\n",
    "                actions, log_probs, v = self.act(states)\n",
    "                \n",
    "                for k in range(self.num_agents):\n",
    "                    if env_info.local_done[k]:\n",
    "                        scores_recorder.append(episodic_scores[k])\n",
    "                        episodic_scores[k] = 0\n",
    "                        \n",
    "            if reward_flag:\n",
    "                print('\\n Current Episode {}! NaN in rewards!'.format(i))\n",
    "            \n",
    "            scores_recorder = np.array(scores_recorder)\n",
    "            states_history.append(torch.tensor(states, dtype=torch.float).to(device))\n",
    "            values_history.append(torch.tensor(v, dtype=torch.float).to(device))\n",
    "            \n",
    "            Advantages = []\n",
    "            advantage = 0\n",
    "            Returns = []\n",
    "            returns = 0\n",
    "            #Calculate advantages\n",
    "            for j in reversed(range(len(states_history)-1)):\n",
    "                if method == 'MC':\n",
    "                    returns = rewards_history[j] + (1-dones_history[j])*returns*self.gamma\n",
    "                else:\n",
    "                    returns = rewards_history[j] + (1-dones_history[j])*values_history[j+1].detach()*self.gamma\n",
    "                Returns.append(returns.view(-1))\n",
    "                delta = rewards_history[j] + (1-dones_history[j])*self.gamma*values_history[j+1].detach() - values_history[j].detach()\n",
    "                advantage = advantage*self.gamma*self.tau*(1-dones_history[j]) + delta\n",
    "                Advantages.append(advantage.view(-1))\n",
    "            Advantages.reverse()\n",
    "            Advantages = torch.stack(Advantages).detach().to(device)\n",
    "            if standardlize == 'row':\n",
    "                Advantages = (Advantages - Advantages.mean(dim=1 ,keepdim=True))/Advantages.std(dim=1, keepdim=True)\n",
    "            elif standardlize == 'whole':\n",
    "                Advantages = (Advantages - Advantages.mean())/Advantages.std()\n",
    "            Advantages = Advantages.view(-1,1)\n",
    "            Returns.reverse()\n",
    "            Returns = torch.stack(Returns).detach().to(device)\n",
    "            Returns = Returns.view(-1,1)\n",
    "            \n",
    "            states_history = torch.cat(states_history[:-1], 0)\n",
    "            actions_history = torch.cat(actions_history, 0)\n",
    "            log_probs_history = torch.cat(log_probs_history, 0)\n",
    "            dones_history = torch.cat(dones_history, 0)\n",
    "            \n",
    "            self.learn(states_history, actions_history, log_probs_history, dones_history, Advantages, Returns)\n",
    "            \n",
    "            states_history = []\n",
    "            actions_history = []\n",
    "            rewards_history = []\n",
    "            log_probs_history = []\n",
    "            dones_history = []\n",
    "            values_history = []\n",
    "            \n",
    "            score_window.append(np.nanmean(scores_recorder))\n",
    "            total_window.append(np.mean(total))\n",
    "            all_rewards.append(np.nanmean(scores_recorder))\n",
    "            print('\\rEpisode {}. Total score {:.4f}, average score {:.4f}, past total average {:.4f}, past average {:.4f}, best {:.4f}, worst {:.4f}'.format(i, np.mean(total), np.mean(scores_recorder), np.mean(total_window),np.mean(score_window), np.max(scores_recorder), np.min(scores_recorder)),end='')\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                self.check()\n",
    "                \n",
    "        np.save('PPO_rewards.npy',np.array(all_rewards))\n",
    "        return all_rewards\n",
    "    \n",
    "agent = PPO_Agent(env=env, beta=0, learning_time=2, batch_size=1024,  constraint = 1.5)\n",
    "#agent.train(n_episode=1000, max_t=3000, standardlize='row', method='TD', load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPO_Actor_Critic(\n",
       "  (actor): Actor(\n",
       "    (fc1_bn): BatchNorm1d(129, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc1): Linear(in_features=129, out_features=1024, bias=True)\n",
       "    (fc2_bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (fc2a_bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc2a): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (fc3_bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc3): Linear(in_features=1024, out_features=20, bias=True)\n",
       "  )\n",
       "  (critic): Critic(\n",
       "    (fc1_bn): BatchNorm1d(129, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc1): Linear(in_features=129, out_features=1024, bias=True)\n",
       "    (fc2_bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (fc2a_bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc2a): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (fc3_bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc3): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.network.load_state_dict(torch.load('Crawler_Checkpoint.pth'))\n",
    "agent.network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 5212.492941924681\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "t = 0\n",
    "while t<1500:\n",
    "    t += 1\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = agent.network.actor(states)              # select an action (for each agent)\n",
    "    actions = actions.detach().cpu().numpy()\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    rewards += np.array(env_info.rewards)                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if any(np.isnan(np.array(rewards.reshape(-1)))):                                  # exit loop if episode finished\n",
    "        rewards[np.isnan(rewards)] = -5\n",
    "        print('NaN appeared')\n",
    "    scores += rewards\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 1781.7002083716216\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "t = 0\n",
    "finished = np.zeros(num_agents)\n",
    "while not all(finished):\n",
    "    t += 1\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = agent.network.actor(states)              # select an action (for each agent)\n",
    "    actions = actions.detach().cpu().numpy()\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = np.array(env_info.rewards)                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if any(np.isnan(np.array(rewards.reshape(-1)))):                                  # exit loop if episode finished\n",
    "        rewards[np.isnan(rewards)] = -5\n",
    "        print('NaN appeared')\n",
    "    scores += (1-finished)*rewards\n",
    "    finished[env_info.local_done] = 1\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1834.05256114, 1828.13262673, 1637.52418523, 1763.39746659,\n",
       "       1900.30259768, 1708.84459253, 1894.97552009, 1823.04778559,\n",
       "       1840.52246955, 1622.97052601, 1611.58258032, 1915.04958901])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
