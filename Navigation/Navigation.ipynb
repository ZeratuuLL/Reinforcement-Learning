{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Loading Environment\n",
    "\n",
    "Before starting running this notebook, please make sure you have finished all installments in the [preparation document](https://github.com/ZeratuuLL/Reinforcement-Learning/blob/master/Navigation/Preparation.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following blocks will initialize the environment. A window should appear where you can see what the agent sees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana.app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Watching a random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "step = 0\n",
    "while True:\n",
    "    step +=1\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}, Total step is {}\".format(score,step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Set up a trainable agent\n",
    "\n",
    "This step you load necessary code to build a agent. To be more specific it would be the network and basic set-ups. You should go to the training jupyter notebook and copy first two blocks in step 2. Here I used PPO agent as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Discrete_Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, fc_units=[256,256,256]):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(Discrete_Actor, self).__init__()\n",
    "        self.fc1=nn.Linear(state_size,fc_units[0])\n",
    "        self.fc2=nn.Linear(fc_units[0],fc_units[1])\n",
    "        self.fc3=nn.Linear(fc_units[1],fc_units[2])\n",
    "        self.fc4=nn.Linear(fc_units[2],action_size)\n",
    "        self.output = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x=F.relu(self.fc1(state))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=F.relu(self.fc3(x))\n",
    "        x=self.output(self.fc4(x))\n",
    "        return(x)\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    '''This critic does not consider action'''\n",
    "    def __init__(self, state_size, hidden=[256,256,256]):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], hidden[2])\n",
    "        self.fc4 = nn.Linear(hidden[2], 1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden[0])\n",
    "        self.bn2 = nn.BatchNorm1d(hidden[1])\n",
    "        self.bn3 = nn.BatchNorm1d(hidden[2])\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = self.bn1(F.relu(self.fc1(state)))\n",
    "        x = self.bn2(F.relu(self.fc2(x)))\n",
    "        x = self.bn3(F.relu(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "Batch_Size = 128\n",
    "GAMMA = 0.99            # discount factor for reward\n",
    "TAU = 0.95              # discount factor for advantage\n",
    "Beta = 0                # Coefficient for KL divergence\n",
    "LR1 = 5e-4              # learning rate \n",
    "LR2 = 5e-4              # learning rate \n",
    "Eps = 0.2               # torelated error of ratio\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, lr1=LR1, lr2=LR2, gamma=GAMMA, tau=TAU, beta=Beta, eps=Eps, method='MC', learning_time=4):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            lr1/lr2 (float): learning rate for actor/critic\n",
    "            tau (float): decay rate for advantage function\n",
    "            gamma (float): decay rate for future rewards\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr1 = lr1\n",
    "        self.lr2 = lr2\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.beta = beta\n",
    "        self.batch_size = Batch_Size\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.actor = Discrete_Actor(state_size, action_size).to(device)\n",
    "        self.critic = Critic(state_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.lr1)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.lr2)\n",
    "        self.critic.eval()\n",
    "        if method in ['MC', 'TD']:\n",
    "            self.method = method\n",
    "        else:\n",
    "            print('Only support MC or TD method. Input not supported. Use MC by default')\n",
    "            self.method = 'MC'\n",
    "        \n",
    "        self.learn_time = learning_time # How many updates for each episode\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Returns actions' probabilities for given state as per current policy.\n",
    "        Also saves the history for later update.\n",
    "        \n",
    "        Inputs:\n",
    "        ==========================\n",
    "        state(float): the current state, which is a tensor\n",
    "        \"\"\"\n",
    "        log_probs = self.actor(state)\n",
    "        return log_probs\n",
    "\n",
    "    def learn(self, states, actions, log_probs, advantages, returns):\n",
    "        \"\"\"Update value parameters using the memory of current episode\"\"\"\n",
    "        \n",
    "        mydata = TensorDataset(states.detach(), actions.detach(), log_probs.detach(), advantages.detach(), returns.detach())\n",
    "        Loader = DataLoader(mydata, batch_size = self.batch_size, shuffle = True)\n",
    "        self.critic.train()\n",
    "        \n",
    "        for _ in range(self.learn_time):\n",
    "            for sampled_states, sampled_actions, sampled_log_probs, sampled_advantages, sampled_returns in iter(Loader):\n",
    "                sampled_actions = sampled_actions.long()\n",
    "                new_log_probs = self.act(sampled_states)\n",
    "                ratio = (new_log_probs - sampled_log_probs).exp().gather(1, sampled_actions)\n",
    "                KL = -new_log_probs.exp()*(new_log_probs - sampled_log_probs)\n",
    "                KL = torch.sum(KL, dim=1, keepdim=True)\n",
    "                \n",
    "                estimated_values = self.critic(sampled_states)\n",
    "                \n",
    "                Actor_Loss = -torch.min(input=ratio*sampled_advantages, other=torch.clamp(ratio, 1-self.eps, 1+self.eps)*sampled_advantages).mean()\n",
    "                Actor_Loss -= self.beta*KL.mean()\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                Actor_Loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 5)\n",
    "                self.actor_optimizer.step()\n",
    "                \n",
    "                Critic_Loss = 0.5*(estimated_values - sampled_returns).pow(2).mean()\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                Critic_Loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 5)\n",
    "                self.critic_optimizer.step()       \n",
    "                \n",
    "        self.critic.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following block, you establish the agent like the first line of code in step 3 in training jupyter noteebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, method='MC', learning_time=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Load the trained agent and watch!\n",
    "\n",
    "Download the trained weights and save it to the same direction as this .ipynb file and you can start watching!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Be sure to load the correct weights\n",
    "agent.actor.load_state_dict(torch.load('PPO_TD_checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)        \n",
    "    action_values = agent.act(state)\n",
    "    action=np.argmax(action_values.cpu().data.numpy())# select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have enjoyed, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
